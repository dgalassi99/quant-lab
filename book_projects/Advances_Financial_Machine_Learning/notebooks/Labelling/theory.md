
# DATA ANALYSIS: LABELLING

In this chapter we learn how to label financial data to retrieve features for supervised learning models.

## The Fixed-Horizon Method

Consider a feature matrix $X$ with $I$ rows drawn from some bars $t = 1, ..., T$ with $T>I$. An observation $X_i$ is assigned to a label $y_i$:

- $y_i = -1$ if $r_{t_{i,0},t_{i,0}+h} < - \tau$
- $y_i = 0$ if $|r_{t_{i,0},t_{i,0}+h}| <= \tau$
- $y_i = 1$ if $r_{t_{i,0},t_{i,0}+h} > - \tau$

where $\tau$ is a constant threshold, $t_{i,0}$ is the index of the bar immediately after $X_i$, $t_{i,0}+h$ is the index of the h-th bar after $t_{i,0}$ and $r_{t_{i,0},t_{i,0}+h}$ the price return over an horizon $h$.

$$ r_{t_{i,0},t_{i,0}+h} = P_{t_{i,0}+h}/P_{t_{i,0}} -1 $$

Great... not really! Usually this method is applied on time bars (which we saw having poor stat. properties). Second, the same threshold $\tau$ is applied regardless the the volatility. 

Can we solve these problmes? Partially by:
- Computing a dynamic threshold $\sigma_{t_{i,0}}$ by computing a rolling exponentially weighted std of returns.
- Using volume/dollar bars
 
Why partially? Because this method does not allow us to introduce a stop-loss (SL) or take-profit (TP) strategies. Hence, this method will result unrealistic in real operations!

## The Triple-Barrier Method 

Label an obs. according to the first barrier it touches. We have:

- Two horizontal barriers representing the touch of TP/SL which label the obs. as +-1
- One vertical barrier touched after a given amount of elapsed bars ($h$) which labels the obs. as 0

We have to note that: (1) to label an obs. we need to take into account the the entire path spanning $[t_{i,0},t_{i,0}+h]$; (2) we denote by $t_{i,1}$ the time of the first touch; (3) the horizontal barriers are not necessarily symmetric.

## Size and Side

Now we want to understand how an ML algo. can learn both side and size of a trade. Ne need to learn the side when we do not have a model to set the sign (long/short) of our position. How do we recognize TP or SL in this situation?

We can apply the TBM direcly on all events with a fixed target...but a full pipeline consists in:

1. Apply a filter to only get some events, for example CUSUM filter
2. Calculate a dynamic target, for example as a the exponential moving volatility (getDailyVol)
3. Define a minimum return to consider an event and use a function to filter out events that do not touch this (getEvents)
4. Now you have a df with t1 indicating the timestamp of what occurs first on those eventd (tp, sl or vertical barrier hit) and the target
5. Apply TBM to get the final labels

## Meta-Labelling

Suppose you now have a model to set the side (long/short). We need now to learn the size of these positions.

We will discuss about this method which is used to build a secondary ML model that learns how to use a primary exogenous model.

In the original TBM:

- No side information --> the model does not know if a trade is short or long
- Horizontal barriers are symmetric
- Agnostic labelling --> +1 means that upper barrier has been hit first (TP for long/SL for short), -1 the opposite

Now we want to enhance this model by adding the side argument. And we can also set asymmetric horizontal barriers:
- ptSL[0] and [1] are the sizes of upper and lower barriers (if you set them to 0 you disable the TP/SL)
- Example:
    - For a long trade (side=+1):
        - Upper barrier = entry price + ptSl[0] * trgt (profit taking)
        - Lower barrier = entry price - ptSl[1] * trgt (stop loss)

    - For a short trade (side=-1):
        - Upper barrier = entry price - ptSl[1] * trgt (stop loss)
        - Lower barrier = entry price + ptSl[0] * trgt (profit taking)

We can update the GetEvents and GetBins functions to have an output of (0,1) to reject or confirm a trade. Basically: 

- Primary *exogenous* model (G): Generates initial trade signals (long/short), so it decides when and what direction to trade --> **side**
    - side is accepted by GetEventsMeta and GetLabelsMeta
- Secondary model (meta-labeler): Uses the labels generated by the exogenous model with side info to predict whether the primary model’s signal is likely to be profitable or not.

G produces the side series (signals) which is feed into the labelling pipeline to generate metalables (0,1) telling if the G signals were good or not. Then we can train a seocndary model to rpedict those metalables 

The secondary model’s job is to filter out bad trades by learning from historical outcomes—keeping only the signals where the expected PnL is positive (label = 1), and ignoring the others (label = 0). 

This two-stage approach can significantly improve overall strategy performance by reducing false positives from the primary model.

## How to Use Meta-Labelling

Once we have a model to determine the side of a bet we need one to determine the size. That is ... *How much money I am willing to bet in this position?*

We have seen that meta-labelling covers this specific question. *When should we specifically use it?*

In binary classification problems we deal with the trade-off between FP and FN errors. The goal is to increase the TP rate but also FP rate will increase.

Definitions:
- FP: positive instances incorrectly classified positive
- FN: negative instances incorrectly classified negative
- TP: positive instances correctly classified positive
- TN: negative instances correctly classified negative
- TPR: True Positive Rate --> positive instances correctly classified/positive instances $TP/(TP+FN)$ --> proportion of positive instances that were correctly classified as positive by the model
- FPR: False Negative Rate --> negative instances incorrecly classfied as positive/negative instances $FP/(FP+TN)$ --> proportion of positive instances that are inaccurately detected as positive
- Precision: $TP/(TP+FP)$ --> correctly classified positive/all classfied as positive
- Recall (TPR): $TP/(TP+FN)$ --> correctly classified positive/number of real positives
- Accuracy: $(TP+TN)/(TP+TN+FN+FP)$ --> correct classficiations/total classifications
- F1 Score = $2/(1/Precision+1/Recall)$
The trade off between TPR and FPR is the key problem of binary classification (usually represented on the ROC curve).

This if we aim at improving the precision by lowering FP, this results in an increase of FN (decrease of Recall/TPR) and viceversa. So the usual metric we want to maximize is their harmonic average: F1 Score. Meta-labelling helps us in achieving this result... *how?*

1. Build a model with high recall and low precision (low FN) --> "do not lose opportunities logic"
2. Apply meta-labelling to the positive predicted by the primary model --> "filter out to keep only good opportunities"
